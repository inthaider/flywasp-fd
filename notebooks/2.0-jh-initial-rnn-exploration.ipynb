{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import scipy.stats as stats\n",
    "from docx import Document\n",
    "from docx.shared import Inches\n",
    "from datetime import datetime\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load in the dataframe\n",
    "pickle_path = \"/Users/Dogar/Desktop/Columbia Material/Summer 2023/Kevin's Research Projects/Data/Pickle Objects/\"\n",
    "\n",
    "# And you can read it back into memory like this:\n",
    "df = pd.read_pickle(pickle_path + 'ff-mw.pkl')\n",
    "df = df.drop('plot', axis = 1)\n",
    "\n",
    "cols = df.columns.tolist()\n",
    "\n",
    "# Rearrange the column names\n",
    "cols.insert(cols.index('F2Wdis') + 1, cols.pop(cols.index('F2Wdis_rate')))\n",
    "\n",
    "# Reindex the DataFrame with the new column order\n",
    "df = df[cols]\n",
    "\n",
    "# Calculating the mean of the 'ANTdis_1', 'ANTdis_2 vars\n",
    "df['ANTdis'] = df[['ANTdis_1', 'ANTdis_2']].mean(axis=1)\n",
    "\n",
    "# Adding the label\n",
    "# create new variable 'start_walk'\n",
    "df['start_walk'] = ((df['walk_backwards'] == 1) & (df['walk_backwards'].shift(1) == 0)).astype(int)\n",
    "\n",
    "# Only keeping in the relevant variables\n",
    "\n",
    "df = df[['Frame', 'Fdis', 'FdisF', 'FdisL', 'Wdis', 'WdisF',\n",
    "       'WdisL', 'Fangle', 'Wangle', 'F2Wdis', 'F2Wdis_rate', 'F2Wangle',\n",
    "       'W2Fangle', 'ANTdis', 'F2W_blob_dis', 'bp_F_delta',\n",
    "       'bp_W_delta', 'ap_F_delta', 'ap_W_delta', 'ant_W_delta', 'file', 'start_walk']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "# Preprocessing #\n",
    "#################\n",
    "\n",
    "# Replacing infinity values with nan\n",
    "df = df.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming df is already loaded\n",
    "# If not, uncomment the line below\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Standardize the data\n",
    "columns_to_scale = ['Fdis', 'FdisF', 'FdisL', 'Wdis', 'WdisF', 'WdisL', 'Fangle',\n",
    "                    'Wangle', 'F2Wdis', 'F2Wdis_rate', 'F2Wangle', 'W2Fangle', \n",
    "                    'ANTdis', 'F2W_blob_dis', 'bp_F_delta', 'bp_W_delta', 'ap_F_delta',\n",
    "                    'ap_W_delta', 'ant_W_delta']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape data into sequences\n",
    "def create_sequences(data, sequence_length=3):\n",
    "    x, y = [], []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        # does iloc include the last index?\n",
    "        # Answer: No, it does not\n",
    "        x.append(data.iloc[i:i+sequence_length].values)\n",
    "        y.append(data.iloc[i+sequence_length]['start_walk'])\n",
    "        print(x, y)\n",
    "    return np.array(x), np.array(y)\n",
    "\n",
    "X_train, Y_train = [], []\n",
    "X_test, Y_test = [], []\n",
    "files = df['file'].unique()\n",
    "\n",
    "for file in files:\n",
    "    file_data = df[df['file'] == file].drop(['Frame', 'file'], axis=1)\n",
    "    \n",
    "    # Create sequences for each file\n",
    "    x, y = create_sequences(file_data)\n",
    "    \n",
    "    # Calculate the split index\n",
    "    train_size = int(len(x) * 2/3)\n",
    "    \n",
    "    # Split the sequences for each file\n",
    "    X_train.extend(x[:train_size])\n",
    "    Y_train.extend(y[:train_size])\n",
    "    X_test.extend(x[train_size:])\n",
    "    Y_test.extend(y[train_size:])\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(Y_train)\n",
    "X_test = np.array(X_test)\n",
    "Y_test = np.array(Y_test)\n",
    "\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data: pd.DataFrame, sequence_length: int = 3) -> tuple:\n",
    "    \"\"\"\n",
    "    Reshape data into sequences of length `sequence_length`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pandas.DataFrame\n",
    "        The data to be reshaped into sequences.\n",
    "    sequence_length : int, optional\n",
    "        The length of each sequence, by default 3.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        A tuple containing two numpy arrays. The first array contains the sequences and the second array contains the corresponding labels.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If `sequence_length` is greater than the length of the data.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> data = pd.DataFrame({'a': [1, 2, 3, 4, 5], 'b': [6, 7, 8, 9, 10]})\n",
    "    >>> x, y = create_sequences(data, sequence_length=3)\n",
    "    >>> print(x)\n",
    "    [[[1 6]\n",
    "      [2 7]\n",
    "      [3 8]]\n",
    "\n",
    "     [[2 7]\n",
    "      [3 8]\n",
    "      [4 9]]\n",
    "\n",
    "     [[3 8]\n",
    "      [4 9]\n",
    "      [5 10]]]\n",
    "    >>> print(y)\n",
    "    [ 9 10  0]\n",
    "\n",
    "    \"\"\"\n",
    "    if sequence_length > len(data):\n",
    "        raise ValueError(\"Sequence length cannot be greater than the length of the data.\")\n",
    "    \n",
    "    x, y = [], []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        x.append(data.iloc[i:i+sequence_length].values)\n",
    "        y.append(data.iloc[i+sequence_length]['start_walk'])\n",
    "    return np.array(x), np.array(y)\n",
    "\n",
    "\n",
    "# Initialize empty lists for training and testing sets\n",
    "X_train, Y_train = [], []\n",
    "X_test, Y_test = [], []\n",
    "\n",
    "# Get unique file names from the dataframe\n",
    "files = df['file'].unique()\n",
    "\n",
    "# Loop through each file\n",
    "for file in files:\n",
    "    # Get the data for the current file and drop the 'Frame' and 'file' columns\n",
    "    file_data = df[df['file'] == file].drop(['Frame', 'file'], axis=1)\n",
    "    \n",
    "    # Create sequences for each file\n",
    "    x, y = create_sequences(file_data)\n",
    "    \n",
    "    # Calculate the split index\n",
    "    train_size = int(len(x) * 2/3)\n",
    "    \n",
    "    # Split the sequences for each file into training and testing sets\n",
    "    X_train.extend(x[:train_size])\n",
    "    Y_train.extend(y[:train_size])\n",
    "    X_test.extend(x[train_size:])\n",
    "    Y_test.extend(y[train_size:])\n",
    "\n",
    "# Convert the lists to numpy arrays\n",
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(Y_train)\n",
    "X_test = np.array(X_test)\n",
    "Y_test = np.array(Y_test)\n",
    "\n",
    "# Print the shapes of the training and testing sets\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fd_flywasp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
