{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Exploration & Analysis\n",
    "**Author: Jibran**<br>\n",
    "**Date: 2023-11-01**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from src.data_preprocess.rnn_data_prep import RNNDataPrep\n",
    "from src.models import rnn_model\n",
    "from src.models.train_eval import train_eval_model\n",
    "from src.models.helpers_rnn import plot_predicted_probabilities\n",
    "from src.utils.utilities import create_config_dict\n",
    "\n",
    "# Set up logging\n",
    "# The logging level can be set to one of the following:\n",
    "# DEBUG - Detailed information, typically of interest only when diagnosing problems.\n",
    "# INFO - Confirmation that things are working as expected.\n",
    "# WARNING - An indication that something unexpected happened, or indicative of some problem in the near future\n",
    "# (e.g. ‘disk space low’). The software is still working as expected.\n",
    "# ERROR - Due to a more serious problem, the software has not been able to perform some function.\n",
    "# CRITICAL - A serious error, indicating that the program itself may be unable to continue running.\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(name)s - %(levelname)s - %(message)s\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Data and/or Get Train/Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data = False\n",
    "save_train_test = False\n",
    "\n",
    "rnn_data_prep = RNNDataPrep(pickle_path=\"../data/interim/ff-mw.pkl\",\n",
    "                            train_test_data_par_dir=\"../data/processed/rnn_input/\")\n",
    "\n",
    "X_train, Y_train, X_test, Y_test = rnn_data_prep.get_rnn_data(\n",
    "    load_train_test=False, sequence_length=5, split_ratio=2/3, save_data=save_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print shapes\n",
    "print(\"X_train shape: \", X_train.shape)\n",
    "print(\"Y_train shape: \", Y_train.shape)\n",
    "print(\"X_test shape: \", X_test.shape)\n",
    "print(\"Y_test shape: \", Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running some checks..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check data imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for data imbalance in Y_train and Y_test\n",
    "# Note that the single feature in Y data is a binary classification\n",
    "# 0: no walk\n",
    "# 1: walk\n",
    "logging.info(\"Checking for data imbalance...\")\n",
    "logging.info(f\"Y_train: {np.unique(Y_train, return_counts=True)}\")\n",
    "logging.info(f\"Y_test: {np.unique(Y_test, return_counts=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for invalid values (NaNs and Infs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_nan_values_X_train = np.isnan(X_train).sum()\n",
    "# num_nan_values_X_test = np.isnan(X_test).sum()\n",
    "# num_nan_values_Y_train = np.isnan(Y_train).sum()\n",
    "# num_nan_values_Y_test = np.isnan(Y_test).sum()\n",
    "# print(f\"Number of NaN values in train X set: {num_nan_values_X_train}\")\n",
    "# print(f\"Number of NaN values in test X set: {num_nan_values_X_test}\")\n",
    "# print(f\"Number of NaN values in train Y set: {num_nan_values_Y_train}\")\n",
    "# print(f\"Number of NaN values in test Y set: {num_nan_values_Y_test}\\n\")\n",
    "\n",
    "# num_inf_values_X_train = np.isinf(X_train).sum()\n",
    "# num_inf_values_X_test = np.isinf(X_test).sum()\n",
    "# num_inf_values_Y_train = np.isinf(Y_train).sum()\n",
    "# num_inf_values_Y_test = np.isinf(Y_test).sum()\n",
    "# print(f\"Number of inf values in train X set: {num_inf_values_X_train}\")\n",
    "# print(f\"Number of inf values in test X set: {num_inf_values_X_test}\")\n",
    "# print(f\"Number of inf values in train Y set: {num_inf_values_Y_train}\")\n",
    "# print(f\"Number of inf values in test Y set: {num_inf_values_Y_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace NaNs and Infs with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train[np.isnan(X_train)] = 0\n",
    "# X_test[np.isnan(X_test)] = 0\n",
    "# Y_train[np.isnan(Y_train)] = 0\n",
    "# Y_test[np.isnan(Y_test)] = 0\n",
    "\n",
    "# X_train[np.isinf(X_train)] = 0\n",
    "# X_test[np.isinf(X_test)] = 0\n",
    "# Y_train[np.isinf(Y_train)] = 0\n",
    "# Y_test[np.isinf(Y_test)] = 0\n",
    "\n",
    "# # handle_inf_na(X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the RNN model\n",
    "print(f\"Training RNN Model...\\n===============================\\n\")\n",
    "# - 1  ### -1 because we drop the target column????\n",
    "input_size = X_train.shape[2]\n",
    "\n",
    "# print(f\"Input size: {input_size}\\n\\n\")\n",
    "hidden_size = 64\n",
    "output_size = 2\n",
    "num_epochs = 2\n",
    "batch_size = 512\n",
    "learning_rate = 0.01\n",
    "batch_first = True\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model, test_labels_and_probs = train_eval_model(X_train, Y_train, X_test, Y_test, input_size,\n",
    "                         hidden_size, output_size, num_epochs, batch_size, learning_rate, device, batch_first=batch_first)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation, Visualization, and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_indices = rnn_data_prep.test_indices\n",
    "df = rnn_data_prep.df\n",
    "\n",
    "plot_df, mean_df = plot_predicted_probabilities(df, test_indices, test_labels_and_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model and config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model name\n",
    "model_architecture = \"rnn\"\n",
    "# get the raw data id, in this case 'ff-mw'\n",
    "raw_data_id = rnn_data_prep.raw_data_id\n",
    "version_number = 1\n",
    "model_name = f\"{model_architecture}_{raw_data_id}_v{version_number}\"\n",
    "\n",
    "# Define/get config details\n",
    "rnn_timestamp = model.timestamp\n",
    "interim_data_path = rnn_data_prep.interim_data_path\n",
    "processed_data_path = rnn_data_prep.processed_data_path\n",
    "\n",
    "# Create the configuration dictionary\n",
    "config = create_config_dict(\n",
    "    model_name=f\"{rnn_timestamp}_{model_name}\",\n",
    "    input_size=input_size,\n",
    "    hidden_size=hidden_size,\n",
    "    output_size=output_size,\n",
    "    num_epochs=num_epochs,\n",
    "    batch_size=batch_size,\n",
    "    learning_rate=learning_rate,\n",
    "    raw_data_path=None,\n",
    "    interim_data_path=interim_data_path,\n",
    "    processed_data_path=processed_data_path,\n",
    "    logging_level='DEBUG',\n",
    "    logging_format='%(asctime)s - %(levelname)s - %(module)s - %(message)s'\n",
    ")\n",
    "\n",
    "# Save the trained model and configuration settings\n",
    "model_dir = Path(f\"models/{model_name}\")\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "config_dir = Path(f\"config/{model_name}\")\n",
    "config_dir.mkdir(parents=True, exist_ok=True)\n",
    "rnn_model.save_model_and_config(model, model_name, rnn_timestamp,\n",
    "                                interim_data_path, processed_data_path, config, model_dir, config_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old/Extra/Misc. Code Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import f1_score\n",
    "# from torch.utils.data import DataLoader, Dataset\n",
    "# import torch.nn as nn\n",
    "# # Initialize a new model\n",
    "# input_size = X_test.shape[2]  # Make sure this is correct\n",
    "# hidden_size = 64\n",
    "# output_size = 2\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# new_model = rnn_model.RNN(input_size=input_size, hidden_size=hidden_size, output_size=output_size, batch_first=True).to(device)\n",
    "\n",
    "# # Load the model\n",
    "# model_path = \"models/rnn_ff-mw_v1/20231020_1701_model_2c92c2793be07eaf3765665d6287ded4_971fce5d8c82c2d1bf8db68939c8162d.pt\"\n",
    "# state_dict = torch.load(model_path)\n",
    "# new_model.load_state_dict(state_dict)\n",
    "# # loaded_model = torch.load(model_path)\n",
    "# new_model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# def evaluate_f1(model, X_test, Y_test, batch_size, device):\n",
    "#     test_dataset = rnn_model.WalkDataset(X_test, Y_test)\n",
    "#     test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "#     model.eval()\n",
    "\n",
    "#     # Initialize running loss & sum of squared gradients and parameters\n",
    "#     running_loss = 0.0\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "    \n",
    "#     y_true = []\n",
    "#     y_pred = []\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for i, (inputs, labels) in enumerate(test_loader):\n",
    "#             inputs, labels = inputs.to(device), labels.to(device)\n",
    "#             outputs = model(inputs)\n",
    "#             # Using CrossEntropyLoss as the loss function\n",
    "#             criterion = nn.CrossEntropyLoss()\n",
    "#             loss = criterion(outputs, labels)  # Compute loss\n",
    "#             running_loss += loss.item()  # Accumulate loss\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "#             total += labels.size(0)  # Accumulate total number of samples\n",
    "#             correct += (predicted == labels).sum().item()\n",
    "#             y_true.extend(labels.cpu().numpy().tolist())\n",
    "#             y_pred.extend(predicted.cpu().numpy().tolist())\n",
    "            \n",
    "#     # Calculate average loss and accuracy over all batches\n",
    "#     test_loss = running_loss / len(test_loader)\n",
    "#     test_acc = correct / total\n",
    "\n",
    "#     print(\n",
    "#         f\"Test Error: \\n Accuracy: {(100*test_acc):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    \n",
    "#     f1 = f1_score(y_true, y_pred)  # You can change the \"average\" parameter to suit your needs\n",
    "\n",
    "#     return f1\n",
    "\n",
    "# # Make sure you load your saved model into the variable `loaded_model`\n",
    "# # Also, ensure X_test, Y_test, batch_size and device are set\n",
    "\n",
    "# f1 = evaluate_f1(new_model, X_test, Y_test, batch_size, device)\n",
    "# print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fd_flywasp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
