{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Performs preprocessing steps on the input DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The input DataFrame.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        The preprocessed DataFrame.\n",
    "    \"\"\"\n",
    "    preprocessor = DataPreprocessor(df=df)\n",
    "    preprocessor.drop_columns([\"plot\"])  # Drop the 'plot' column\n",
    "    preprocessor.calculate_means([[\"ANTdis_1\", \"ANTdis_2\"]], [\"ANTdis\"])  # Calculate the mean of 'ANTdis_1' and 'ANTdis_2' and store it in a new column 'ANTdis'\n",
    "    preprocessor.add_labels([\"walk_backwards\", \"walk_backwards\"], \"start_walk\")  # Add a new column 'start_walk' with value 'walk_backwards' for rows where the 'walk_backwards' column has value 'walk_backwards'\n",
    "    preprocessor.handle_infinity_and_na()  # Replace infinity and NaN values with appropriate values\n",
    "    preprocessor.specific_rearrange(\n",
    "        \"F2Wdis_rate\", \"F2Wdis\"\n",
    "    )  # Rearrange the column names\n",
    "    preprocessor.rearrange_columns(\n",
    "        [\n",
    "            \"Frame\",\n",
    "            \"Fdis\",\n",
    "            \"FdisF\",\n",
    "            \"FdisL\",\n",
    "            \"Wdis\",\n",
    "            \"WdisF\",\n",
    "            \"WdisL\",\n",
    "            \"Fangle\",\n",
    "            \"Wangle\",\n",
    "            \"F2Wdis\",\n",
    "            \"F2Wdis_rate\",\n",
    "            \"F2Wangle\",\n",
    "            \"W2Fangle\",\n",
    "            \"ANTdis\",\n",
    "            \"F2W_blob_dis\",\n",
    "            \"bp_F_delta\",\n",
    "            \"bp_W_delta\",\n",
    "            \"ap_F_delta\",\n",
    "            \"ap_W_delta\",\n",
    "            \"ant_W_delta\",\n",
    "            \"file\",\n",
    "            \"start_walk\",\n",
    "        ]\n",
    "    )  # Rearrange the columns in a specific order\n",
    "    return preprocessor.df\n",
    "\n",
    "\n",
    "def engineer_features(df):\n",
    "    \"\"\"\n",
    "    Performs feature engineering steps on the input DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The input DataFrame.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        The feature-engineered DataFrame.\n",
    "    \"\"\"\n",
    "    feature_engineer = FeatureEngineer(df=df)\n",
    "    feature_engineer.standardize_features(\n",
    "        [\n",
    "            \"Fdis\",\n",
    "            \"FdisF\",\n",
    "            \"FdisL\",\n",
    "            \"Wdis\",\n",
    "            \"WdisF\",\n",
    "            \"WdisL\",\n",
    "            \"Fangle\",\n",
    "            \"Wangle\",\n",
    "            \"F2Wdis\",\n",
    "            \"F2Wdis_rate\",\n",
    "            \"F2Wangle\",\n",
    "            \"W2Fangle\",\n",
    "            \"ANTdis\",\n",
    "            \"F2W_blob_dis\",\n",
    "            \"bp_F_delta\",\n",
    "            \"bp_W_delta\",\n",
    "            \"ap_F_delta\",\n",
    "            \"ap_W_delta\",\n",
    "            \"ant_W_delta\",\n",
    "        ]\n",
    "    )  # Standardize the selected features\n",
    "    return feature_engineer.df\n",
    "\n",
    "\n",
    "def train_model(X_train, Y_train, X_test, Y_test, input_size, hidden_size, output_size, num_epochs, batch_size, learning_rate, device, batch_first=True):\n",
    "    \"\"\"\n",
    "    Trains an RNN model on the input data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : numpy.ndarray\n",
    "        The training input sequences.\n",
    "    Y_train : numpy.ndarray\n",
    "        The training target sequences.\n",
    "    X_test : numpy.ndarray\n",
    "        The test input sequences.\n",
    "    Y_test : numpy.ndarray\n",
    "        The test target sequences.\n",
    "    input_size : int\n",
    "        The size of the input features.\n",
    "    hidden_size : int\n",
    "        The size of the hidden layer.\n",
    "    output_size : int\n",
    "        The size of the output layer.\n",
    "    num_epochs : int\n",
    "        The number of training epochs.\n",
    "    batch_size : int\n",
    "        The batch size for training.\n",
    "    learning_rate : float\n",
    "        The learning rate for training.\n",
    "    device : torch.device\n",
    "        The device to use for training.\n",
    "    batch_first : bool, optional\n",
    "        Whether the input sequences have the batch dimension as the first dimension.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.nn.Module\n",
    "        The trained RNN model.\n",
    "    \"\"\"\n",
    "    model = train_rnn_model(X_train, Y_train, X_test, Y_test, input_size,\n",
    "                            hidden_size, output_size, num_epochs, batch_size, learning_rate, device, batch_first=batch_first)  # Train the RNN model\n",
    "    return model\n",
    "\n",
    "\n",
    "def save_model_and_config(model, model_name, timestamp, pickle_path, processed_data_path, config, model_dir, config_dir):\n",
    "    \"\"\"\n",
    "    Saves the trained model and configuration settings.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : torch.nn.Module\n",
    "        The trained RNN model.\n",
    "    model_name : str\n",
    "        The name of the model.\n",
    "    timestamp : str\n",
    "        The timestamp to use in the output file names.\n",
    "    pickle_path : str\n",
    "        The path to the input data pickle file.\n",
    "    processed_data_path : str\n",
    "        The path to the processed data pickle file.\n",
    "    config : dict\n",
    "        The configuration settings for the model.\n",
    "    model_dir : pathlib.Path\n",
    "        The directory to save the trained model.\n",
    "    config_dir : pathlib.Path\n",
    "        The directory to save the configuration settings.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Get the hash values of the model and configuration\n",
    "    model_hash = hashlib.md5(str(model.state_dict()).encode('utf-8')).hexdigest()\n",
    "    config_hash = hashlib.md5(str(config).encode('utf-8')).hexdigest()\n",
    "\n",
    "    # Check if the model and configuration already exist\n",
    "    existing_models = [f.name for f in model_dir.glob(\"*.pt\")]\n",
    "    existing_configs = [f.name for f in config_dir.glob(\"*.yaml\")]\n",
    "    if f\"rnn_model_{model_hash}.pt\" in existing_models and f\"config_{config_hash}.yaml\" in existing_configs:\n",
    "        logging.info(\"Model and configuration already exist. Skipping saving.\")\n",
    "    else:\n",
    "        # Save the trained model\n",
    "        model_path = model_dir / \\\n",
    "            f\"{timestamp}_model_{model_hash}_{config_hash}.pt\"\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "        # Save the configuration settings\n",
    "        config_path = config_dir / f\"{timestamp}_config_{config_hash}.yaml\"\n",
    "        with open(config_path, \"w\") as f:\n",
    "            yaml.dump(config, f)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function that performs data preprocessing, feature engineering, model training, and model saving.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Set up logging\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    # Initialize preprocessing object and load data\n",
    "    pickle_path = \"data/interim/ff-mw.pkl\"\n",
    "    preprocessor = DataPreprocessor(pickle_path=pickle_path)\n",
    "    logging.info(\"Loading data...\")\n",
    "    df = preprocessor.load_data()\n",
    "\n",
    "    # Perform preprocessing steps\n",
    "    logging.info(\"Performing preprocessing steps...\")\n",
    "    df = preprocess_data(df)\n",
    "\n",
    "    # Perform feature engineering steps\n",
    "    logging.info(\"Performing feature engineering steps...\")\n",
    "    df = engineer_features(df)\n",
    "\n",
    "    # Save the processed data\n",
    "    logging.info(\"Saving processed data...\")\n",
    "    input_data = \"ff-mw\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "    processed_data_path = preprocessor.save_processed_data(input_data, timestamp)  # Save the processed data to a file\n",
    "\n",
    "    # Prepare sequences and train-test splits\n",
    "    logging.info(\"Preparing sequences and train-test splits...\")\n",
    "    X_train, Y_train, X_test, Y_test = prepare_train_test_sequences(df)\n",
    "\n",
    "    # Train the RNN model\n",
    "    logging.info(\"Training RNN model...\")\n",
    "    input_size = X_train.shape[2] - 1  # -1 because we drop the target column\n",
    "    hidden_size = 64\n",
    "    output_size = 2\n",
    "    num_epochs = 10\n",
    "    batch_size = 32\n",
    "    learning_rate = 0.001\n",
    "    batch_first = True\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = train_model(X_train, Y_train, X_test, Y_test, input_size,\n",
    "                        hidden_size, output_size, num_epochs, batch_size, learning_rate, device, batch_first=batch_first)\n",
    "\n",
    "    # Create the model name\n",
    "    model_architecture = \"rnn\"\n",
    "    version_number = 1\n",
    "    model_name = f\"{model_architecture}_{input_data}_v{version_number}\"\n",
    "\n",
    "    # Create the configuration dictionary\n",
    "    config = create_config_dict(\n",
    "        model_name=f\"{timestamp}_{model_name}\",\n",
    "        input_size=input_size,\n",
    "        hidden_size=hidden_size,\n",
    "        output_size=output_size,\n",
    "        num_epochs=num_epochs,\n",
    "        batch_size=batch_size,\n",
    "        learning_rate=learning_rate,\n",
    "        raw_data_path=None,\n",
    "        interim_data_path=pickle_path,\n",
    "        processed_data_path=processed_data_path,\n",
    "        logging_level='DEBUG',\n",
    "        logging_format='%(asctime)s - %(levelname)s - %(module)s - %(message)s'\n",
    "    )  # Create a dictionary with configuration settings\n",
    "\n",
    "    # Save the trained model and configuration settings\n",
    "    model_dir = Path(f\"models/{model_name}\")\n",
    "    model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    config_dir = Path(f\"config/{model_name}\")\n",
    "    config_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    save_model_and_config(model, model_name, timestamp, pickle_path, processed_data_path, config, model_dir, config_dir)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
