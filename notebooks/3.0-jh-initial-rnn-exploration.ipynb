{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import torch\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from src.data_preprocess.preprocessing import DataPreprocessor\n",
    "from src.data_preprocess.feature_engineering import FeatureEngineer\n",
    "from src.utils.utilities import prepare_train_test_sequences\n",
    "from src.utils.utilities import create_config_dict\n",
    "from src.utils.utilities import get_hash\n",
    "from src.utils.utilities import load_train_test_data\n",
    "from src.utils.utilities import handle_infinity_and_na_numpy as handle_inf_na\n",
    "from src.models.rnn_model import train_rnn_model\n",
    "from src.models import rnn_model\n",
    "import hashlib\n",
    "import pickle\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Performs preprocessing steps on the input DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The input DataFrame.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        The preprocessed DataFrame.\n",
    "    \"\"\"\n",
    "    preprocessor = DataPreprocessor(df=df)\n",
    "    preprocessor.drop_columns([\"plot\"])  # Drop the 'plot' column\n",
    "    preprocessor.calculate_means([[\"ANTdis_1\", \"ANTdis_2\"]], [\"ANTdis\"])  # Calculate the mean of 'ANTdis_1' and 'ANTdis_2' and store it in a new column 'ANTdis'\n",
    "    preprocessor.add_labels([\"walk_backwards\", \"walk_backwards\"], \"start_walk\")  # Add a new column 'start_walk' with value 'walk_backwards' for rows where the 'walk_backwards' column has value 'walk_backwards'\n",
    "    preprocessor.handle_infinity_and_na()  # Replace infinity and NaN values with appropriate values\n",
    "    preprocessor.specific_rearrange(\n",
    "        \"F2Wdis_rate\", \"F2Wdis\"\n",
    "    )  # Rearrange the column names\n",
    "    preprocessor.rearrange_columns(\n",
    "        [\n",
    "            \"Frame\",\n",
    "            \"Fdis\",\n",
    "            \"FdisF\",\n",
    "            \"FdisL\",\n",
    "            \"Wdis\",\n",
    "            \"WdisF\",\n",
    "            \"WdisL\",\n",
    "            \"Fangle\",\n",
    "            \"Wangle\",\n",
    "            \"F2Wdis\",\n",
    "            \"F2Wdis_rate\",\n",
    "            \"F2Wangle\",\n",
    "            \"W2Fangle\",\n",
    "            \"ANTdis\",\n",
    "            \"F2W_blob_dis\",\n",
    "            \"bp_F_delta\",\n",
    "            \"bp_W_delta\",\n",
    "            \"ap_F_delta\",\n",
    "            \"ap_W_delta\",\n",
    "            \"ant_W_delta\",\n",
    "            \"file\",\n",
    "            \"start_walk\",\n",
    "        ]\n",
    "    )  # Rearrange the columns in a specific order\n",
    "    return preprocessor.df\n",
    "\n",
    "\n",
    "def engineer_features(df):\n",
    "    \"\"\"\n",
    "    Performs feature engineering steps on the input DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The input DataFrame.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        The feature-engineered DataFrame.\n",
    "    \"\"\"\n",
    "    feature_engineer = FeatureEngineer(df=df)\n",
    "    feature_engineer.standardize_features(\n",
    "        [\n",
    "            \"Fdis\",\n",
    "            \"FdisF\",\n",
    "            \"FdisL\",\n",
    "            \"Wdis\",\n",
    "            \"WdisF\",\n",
    "            \"WdisL\",\n",
    "            \"Fangle\",\n",
    "            \"Wangle\",\n",
    "            \"F2Wdis\",\n",
    "            \"F2Wdis_rate\",\n",
    "            \"F2Wangle\",\n",
    "            \"W2Fangle\",\n",
    "            \"ANTdis\",\n",
    "            \"F2W_blob_dis\",\n",
    "            \"bp_F_delta\",\n",
    "            \"bp_W_delta\",\n",
    "            \"ap_F_delta\",\n",
    "            \"ap_W_delta\",\n",
    "            \"ant_W_delta\",\n",
    "        ]\n",
    "    )  # Standardize the selected features\n",
    "    return feature_engineer.df\n",
    "\n",
    "\n",
    "def train_model(X_train, Y_train, X_test, Y_test, input_size, hidden_size, output_size, num_epochs, batch_size, learning_rate, device, batch_first=True):\n",
    "    \"\"\"\n",
    "    Trains an RNN model on the input data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : numpy.ndarray\n",
    "        The training input sequences.\n",
    "    Y_train : numpy.ndarray\n",
    "        The training target sequences.\n",
    "    X_test : numpy.ndarray\n",
    "        The test input sequences.\n",
    "    Y_test : numpy.ndarray\n",
    "        The test target sequences.\n",
    "    input_size : int\n",
    "        The size of the input features.\n",
    "    hidden_size : int\n",
    "        The size of the hidden layer.\n",
    "    output_size : int\n",
    "        The size of the output layer.\n",
    "    num_epochs : int\n",
    "        The number of training epochs.\n",
    "    batch_size : int\n",
    "        The batch size for training.\n",
    "    learning_rate : float\n",
    "        The learning rate for training.\n",
    "    device : torch.device\n",
    "        The device to use for training.\n",
    "    batch_first : bool, optional\n",
    "        Whether the input sequences have the batch dimension as the first dimension.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.nn.Module\n",
    "        The trained RNN model.\n",
    "    \"\"\"\n",
    "    model = train_rnn_model(X_train, Y_train, X_test, Y_test, input_size,\n",
    "                            hidden_size, output_size, num_epochs, batch_size, learning_rate, device, batch_first=batch_first)  # Train the RNN model\n",
    "    return model\n",
    "\n",
    "\n",
    "def save_model_and_config(model, model_name, timestamp, pickle_path, processed_data_path, config, model_dir, config_dir):\n",
    "    \"\"\"\n",
    "    Saves the trained model and configuration settings.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : torch.nn.Module\n",
    "        The trained RNN model.\n",
    "    model_name : str\n",
    "        The name of the model.\n",
    "    timestamp : str\n",
    "        The timestamp to use in the output file names.\n",
    "    pickle_path : str\n",
    "        The path to the input data pickle file.\n",
    "    processed_data_path : str\n",
    "        The path to the processed data pickle file.\n",
    "    config : dict\n",
    "        The configuration settings for the model.\n",
    "    model_dir : pathlib.Path\n",
    "        The directory to save the trained model.\n",
    "    config_dir : pathlib.Path\n",
    "        The directory to save the configuration settings.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Get the hash values of the model and configuration\n",
    "    model_hash = hashlib.md5(str(model.state_dict()).encode('utf-8')).hexdigest()\n",
    "    config_hash = hashlib.md5(str(config).encode('utf-8')).hexdigest()\n",
    "\n",
    "    # Check if the model and configuration already exist\n",
    "    existing_models = [f.name for f in model_dir.glob(\"*.pt\")]\n",
    "    existing_configs = [f.name for f in config_dir.glob(\"*.yaml\")]\n",
    "    if f\"rnn_model_{model_hash}.pt\" in existing_models and f\"config_{config_hash}.yaml\" in existing_configs:\n",
    "        logging.info(\"Model and configuration already exist. Skipping saving.\")\n",
    "    else:\n",
    "        # Save the trained model\n",
    "        model_path = model_dir / \\\n",
    "            f\"{timestamp}_model_{model_hash}_{config_hash}.pt\"\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "        # Save the configuration settings\n",
    "        config_path = config_dir / f\"{timestamp}_config_{config_hash}.yaml\"\n",
    "        with open(config_path, \"w\") as f:\n",
    "            yaml.dump(config, f)\n",
    "\n",
    "def save_train_test_data(X_train, Y_train, X_test, Y_test):\n",
    "    \"\"\"\n",
    "    Saves the train and test datasets for the RNN model as .pkl files.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : numpy.ndarray\n",
    "        The training input sequences.\n",
    "    Y_train : numpy.ndarray\n",
    "        The training target values.\n",
    "    X_test : numpy.ndarray\n",
    "        The testing input sequences.\n",
    "    Y_test : numpy.ndarray\n",
    "        The testing target values.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create a timestamped directory for the processed data\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d\")\n",
    "        dir_name = Path(f\"data/processed/rnn_input/{timestamp}\")\n",
    "        dir_name.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Save the train and test datasets as .pkl files\n",
    "        X_train_file = dir_name / \"X_train.pkl\"\n",
    "        Y_train_file = dir_name / \"Y_train.pkl\"\n",
    "        X_test_file = dir_name / \"X_test.pkl\"\n",
    "        Y_test_file = dir_name / \"Y_test.pkl\"\n",
    "\n",
    "        with open(X_train_file, \"wb\") as f:\n",
    "            pickle.dump(X_train, f)\n",
    "        with open(Y_train_file, \"wb\") as f:\n",
    "            pickle.dump(Y_train, f)\n",
    "        with open(X_test_file, \"wb\") as f:\n",
    "            pickle.dump(X_test, f)\n",
    "        with open(Y_test_file, \"wb\") as f:\n",
    "            pickle.dump(Y_test, f)\n",
    "\n",
    "        logging.info(f\"Saved train and test datasets to {dir_name}.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving train and test datasets: {e}\")\n",
    "        raise\n",
    "    return dir_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_load = True\n",
    "if is_load:\n",
    "    X_train, Y_train, X_test, Y_test = load_train_test_data('../data/processed/rnn_input/')\n",
    "else:\n",
    "    # Initialize preprocessing object and load data\n",
    "    pickle_path = \"data/interim/ff-mw.pkl\"\n",
    "    preprocessor = DataPreprocessor(pickle_path=pickle_path)\n",
    "    logging.info(\"Loading data...\")\n",
    "    df = preprocessor.load_data()\n",
    "\n",
    "    # Perform preprocessing steps\n",
    "    logging.info(\"Performing preprocessing steps...\")\n",
    "    df = preprocess_data(df)\n",
    "\n",
    "    # Perform feature engineering steps\n",
    "    logging.info(\"Performing feature engineering steps...\")\n",
    "    df = engineer_features(df)\n",
    "\n",
    "    # Save the processed data\n",
    "    logging.info(\"Saving processed data...\")\n",
    "    input_data = \"ff-mw\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "    processed_data_path = preprocessor.save_processed_data(input_data, timestamp)  # Save the processed data to a file\n",
    "\n",
    "    # Prepare sequences and train-test splits\n",
    "    logging.info(\"Preparing sequences and train-test splits...\")\n",
    "    X_train, Y_train, X_test, Y_test = prepare_train_test_sequences(df)\n",
    "\n",
    "    # Save the train-test splits\n",
    "    logging.info(\"Saving train-test splits...\")\n",
    "    save_train_test_data(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for data imbalance in Y_train and Y_test\n",
    "# Note that the single feature in Y data is a binary classification\n",
    "# 0: no walk\n",
    "# 1: walk\n",
    "logging.info(\"Checking for data imbalance...\")\n",
    "logging.info(f\"Y_train: {np.unique(Y_train, return_counts=True)}\")\n",
    "logging.info(f\"Y_test: {np.unique(Y_test, return_counts=True)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6527728, 2, 19) (3264000, 2, 19) (6527728,) (3264000,)\n",
      "Number of NaN values in train X set: 119625878\n",
      "Number of NaN values in test X set: 49944411\n",
      "Number of NaN values in train Y set: 0\n",
      "Number of NaN values in test Y set: 0\n",
      "Number of inf values in train X set: 135506\n",
      "Number of inf values in test X set: 116331\n",
      "Number of inf values in train Y set: 0\n",
      "Number of inf values in test Y set: 0\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\n",
    "\n",
    "num_nan_values_X_train = np.isnan(X_train).sum()\n",
    "num_nan_values_X_test = np.isnan(X_test).sum()\n",
    "num_nan_values_Y_train = np.isnan(Y_train).sum()\n",
    "num_nan_values_Y_test = np.isnan(Y_test).sum()\n",
    "print(f\"Number of NaN values in train X set: {num_nan_values_X_train}\")\n",
    "print(f\"Number of NaN values in test X set: {num_nan_values_X_test}\")\n",
    "print(f\"Number of NaN values in train Y set: {num_nan_values_Y_train}\")\n",
    "print(f\"Number of NaN values in test Y set: {num_nan_values_Y_test}\")\n",
    "\n",
    "num_inf_values_X_train = np.isinf(X_train).sum()\n",
    "num_inf_values_X_test = np.isinf(X_test).sum()\n",
    "num_inf_values_Y_train = np.isinf(Y_train).sum()\n",
    "num_inf_values_Y_test = np.isinf(Y_test).sum()\n",
    "print(f\"Number of inf values in train X set: {num_inf_values_X_train}\")\n",
    "print(f\"Number of inf values in test X set: {num_inf_values_X_test}\")\n",
    "print(f\"Number of inf values in train Y set: {num_inf_values_Y_train}\")\n",
    "print(f\"Number of inf values in test Y set: {num_inf_values_Y_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[np.isnan(X_train)] = 0\n",
    "X_test[np.isnan(X_test)] = 0\n",
    "Y_train[np.isnan(Y_train)] = 0\n",
    "Y_test[np.isnan(Y_test)] = 0\n",
    "\n",
    "X_train[np.isinf(X_train)] = 0\n",
    "X_test[np.isinf(X_test)] = 0\n",
    "Y_train[np.isinf(Y_train)] = 0\n",
    "Y_test[np.isinf(Y_test)] = 0\n",
    "# handle_inf_na(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RNN Model...\n",
      "===============================\n",
      "\n",
      "Input size: 19\n",
      "\n",
      "\n",
      "Epoch 1/10\n",
      "-------------------------------\n",
      "Print interval: 1275\n",
      "Number of batches: 12750\n",
      "Batch size: 512\n",
      "Loss: 0.767469  [  512/6527728]\n",
      "Loss: 0.329938  [653312/6527728]\n",
      "Loss: 0.341922  [1306112/6527728]\n",
      "Loss: 0.320609  [1958912/6527728]\n",
      "Loss: 0.318394  [2611712/6527728]\n",
      "Loss: 0.326418  [3264512/6527728]\n",
      "Loss: 0.332280  [3917312/6527728]\n",
      "Loss: 0.319775  [4570112/6527728]\n",
      "Loss: 0.317503  [5222912/6527728]\n",
      "Loss: 0.317598  [5875712/6527728]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Sum squared grads/params in Epoch 1:\n",
      "\tSum of squared gradients :    2152.8122\n",
      "\tSum of squared parameters:  425310.0573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Error: \n",
      " Avg loss: 0.343351\n",
      "Test Error: \n",
      " Accuracy: 99.9%, Avg loss: 0.322466 \n",
      "\n",
      "Epoch 2/10\n",
      "-------------------------------\n",
      "Print interval: 1275\n",
      "Number of batches: 12750\n",
      "Batch size: 512\n",
      "Loss: 0.317434  [  512/6527728]\n",
      "Loss: 0.314492  [653312/6527728]\n",
      "Loss: 0.318075  [1306112/6527728]\n",
      "Loss: 0.314417  [1958912/6527728]\n",
      "Loss: 0.314329  [2611712/6527728]\n",
      "Loss: 0.317415  [3264512/6527728]\n",
      "Loss: 0.320617  [3917312/6527728]\n",
      "Loss: 0.318167  [4570112/6527728]\n",
      "Loss: 0.314764  [5222912/6527728]\n",
      "Loss: 0.315246  [5875712/6527728]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Sum squared grads/params in Epoch 2:\n",
      "\tSum of squared gradients :      21.0592\n",
      "\tSum of squared parameters:  438541.4672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Error: \n",
      " Avg loss: 0.319433\n",
      "Test Error: \n",
      " Accuracy: 99.9%, Avg loss: 0.318319 \n",
      "\n",
      "Epoch 3/10\n",
      "-------------------------------\n",
      "Print interval: 1275\n",
      "Number of batches: 12750\n",
      "Batch size: 512\n",
      "Loss: 0.314840  [  512/6527728]\n",
      "Loss: 0.313823  [653312/6527728]\n",
      "Loss: 0.315504  [1306112/6527728]\n",
      "Loss: 0.313783  [1958912/6527728]\n",
      "Loss: 0.313778  [2611712/6527728]\n",
      "Loss: 0.315514  [3264512/6527728]\n",
      "Loss: 0.317493  [3917312/6527728]\n",
      "Loss: 0.317741  [4570112/6527728]\n",
      "Loss: 0.314067  [5222912/6527728]\n",
      "Loss: 0.314475  [5875712/6527728]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Sum squared grads/params in Epoch 3:\n",
      "\tSum of squared gradients :       7.9613\n",
      "\tSum of squared parameters:  446251.3208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Error: \n",
      " Avg loss: 0.316976\n",
      "Test Error: \n",
      " Accuracy: 99.9%, Avg loss: 0.316845 \n",
      "\n",
      "Epoch 4/10\n",
      "-------------------------------\n",
      "Print interval: 1275\n",
      "Number of batches: 12750\n",
      "Batch size: 512\n",
      "Loss: 0.314140  [  512/6527728]\n",
      "Loss: 0.313603  [653312/6527728]\n",
      "Loss: 0.314628  [1306112/6527728]\n",
      "Loss: 0.313594  [1958912/6527728]\n",
      "Loss: 0.313596  [2611712/6527728]\n",
      "Loss: 0.314762  [3264512/6527728]\n",
      "Loss: 0.316155  [3917312/6527728]\n",
      "Loss: 0.317569  [4570112/6527728]\n",
      "Loss: 0.313802  [5222912/6527728]\n",
      "Loss: 0.314126  [5875712/6527728]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Sum squared grads/params in Epoch 4:\n",
      "\tSum of squared gradients :       3.7313\n",
      "\tSum of squared parameters:  451690.9150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Error: \n",
      " Avg loss: 0.315992\n",
      "Test Error: \n",
      " Accuracy: 99.9%, Avg loss: 0.316121 \n",
      "\n",
      "Epoch 5/10\n",
      "-------------------------------\n",
      "Print interval: 1275\n",
      "Number of batches: 12750\n",
      "Batch size: 512\n",
      "Loss: 0.313854  [  512/6527728]\n",
      "Loss: 0.313510  [653312/6527728]\n",
      "Loss: 0.314224  [1306112/6527728]\n",
      "Loss: 0.313504  [1958912/6527728]\n",
      "Loss: 0.313505  [2611712/6527728]\n",
      "Loss: 0.314366  [3264512/6527728]\n",
      "Loss: 0.315424  [3917312/6527728]\n",
      "Loss: 0.317474  [4570112/6527728]\n",
      "Loss: 0.313660  [5222912/6527728]\n",
      "Loss: 0.313926  [5875712/6527728]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Sum squared grads/params in Epoch 5:\n",
      "\tSum of squared gradients :       2.3789\n",
      "\tSum of squared parameters:  455903.4555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Error: \n",
      " Avg loss: 0.315472\n",
      "Test Error: \n",
      " Accuracy: 99.9%, Avg loss: 0.315689 \n",
      "\n",
      "Epoch 6/10\n",
      "-------------------------------\n",
      "Print interval: 1275\n",
      "Number of batches: 12750\n",
      "Batch size: 512\n",
      "Loss: 0.313700  [  512/6527728]\n",
      "Loss: 0.313455  [653312/6527728]\n",
      "Loss: 0.313993  [1306112/6527728]\n",
      "Loss: 0.313451  [1958912/6527728]\n",
      "Loss: 0.313452  [2611712/6527728]\n",
      "Loss: 0.314126  [3264512/6527728]\n",
      "Loss: 0.314971  [3917312/6527728]\n",
      "Loss: 0.317414  [4570112/6527728]\n",
      "Loss: 0.313574  [5222912/6527728]\n",
      "Loss: 0.313797  [5875712/6527728]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Sum squared grads/params in Epoch 6:\n",
      "\tSum of squared gradients :       1.8266\n",
      "\tSum of squared parameters:  459331.9087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Error: \n",
      " Avg loss: 0.315141\n",
      "Test Error: \n",
      " Accuracy: 99.9%, Avg loss: 0.315367 \n",
      "\n",
      "Epoch 7/10\n",
      "-------------------------------\n",
      "Print interval: 1275\n",
      "Number of batches: 12750\n",
      "Batch size: 512\n",
      "Loss: 0.313606  [  512/6527728]\n",
      "Loss: 0.313420  [653312/6527728]\n",
      "Loss: 0.313846  [1306112/6527728]\n",
      "Loss: 0.313417  [1958912/6527728]\n",
      "Loss: 0.313417  [2611712/6527728]\n",
      "Loss: 0.313967  [3264512/6527728]\n",
      "Loss: 0.314667  [3917312/6527728]\n",
      "Loss: 0.317374  [4570112/6527728]\n",
      "Loss: 0.313518  [5222912/6527728]\n",
      "Loss: 0.313709  [5875712/6527728]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Sum squared grads/params in Epoch 7:\n",
      "\tSum of squared gradients :       1.2419\n",
      "\tSum of squared parameters:  462186.9147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Error: \n",
      " Avg loss: 0.314909\n",
      "Test Error: \n",
      " Accuracy: 99.9%, Avg loss: 0.315168 \n",
      "\n",
      "Epoch 8/10\n",
      "-------------------------------\n",
      "Print interval: 1275\n",
      "Number of batches: 12750\n",
      "Batch size: 512\n",
      "Loss: 0.313544  [  512/6527728]\n",
      "Loss: 0.313396  [653312/6527728]\n",
      "Loss: 0.313747  [1306112/6527728]\n",
      "Loss: 0.313393  [1958912/6527728]\n",
      "Loss: 0.313393  [2611712/6527728]\n",
      "Loss: 0.313855  [3264512/6527728]\n",
      "Loss: 0.314450  [3917312/6527728]\n",
      "Loss: 0.317344  [4570112/6527728]\n",
      "Loss: 0.313478  [5222912/6527728]\n",
      "Loss: 0.313645  [5875712/6527728]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Sum squared grads/params in Epoch 8:\n",
      "\tSum of squared gradients :       0.8808\n",
      "\tSum of squared parameters:  464643.0558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Error: \n",
      " Avg loss: 0.314760\n",
      "Test Error: \n",
      " Accuracy: 99.9%, Avg loss: 0.315028 \n",
      "\n",
      "Epoch 9/10\n",
      "-------------------------------\n",
      "Print interval: 1275\n",
      "Number of batches: 12750\n",
      "Batch size: 512\n",
      "Loss: 0.313500  [  512/6527728]\n",
      "Loss: 0.313378  [653312/6527728]\n",
      "Loss: 0.313674  [1306112/6527728]\n",
      "Loss: 0.313376  [1958912/6527728]\n",
      "Loss: 0.313375  [2611712/6527728]\n",
      "Loss: 0.313772  [3264512/6527728]\n",
      "Loss: 0.314288  [3917312/6527728]\n",
      "Loss: 0.317322  [4570112/6527728]\n",
      "Loss: 0.313449  [5222912/6527728]\n",
      "Loss: 0.313597  [5875712/6527728]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Sum squared grads/params in Epoch 9:\n",
      "\tSum of squared gradients :       0.6693\n",
      "\tSum of squared parameters:  466808.4121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Error: \n",
      " Avg loss: 0.314650\n",
      "Test Error: \n",
      " Accuracy: 99.9%, Avg loss: 0.314921 \n",
      "\n",
      "Epoch 10/10\n",
      "-------------------------------\n",
      "Print interval: 1275\n",
      "Number of batches: 12750\n",
      "Batch size: 512\n",
      "Loss: 0.313467  [  512/6527728]\n",
      "Loss: 0.313365  [653312/6527728]\n",
      "Loss: 0.313620  [1306112/6527728]\n",
      "Loss: 0.313363  [1958912/6527728]\n",
      "Loss: 0.313362  [2611712/6527728]\n",
      "Loss: 0.313709  [3264512/6527728]\n",
      "Loss: 0.314162  [3917312/6527728]\n",
      "Loss: 0.317305  [4570112/6527728]\n",
      "Loss: 0.313426  [5222912/6527728]\n",
      "Loss: 0.313558  [5875712/6527728]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Sum squared grads/params in Epoch 10:\n",
      "\tSum of squared gradients :       0.5253\n",
      "\tSum of squared parameters:  468744.1679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Error: \n",
      " Avg loss: 0.314566\n",
      "Test Error: \n",
      " Accuracy: 99.9%, Avg loss: 0.314837 \n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'timestamp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/JawanHaider/Documents/Projects/Collaborative/Dogar/flywasp-fd/notebooks/3.0-jh-initial-rnn-exploration.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/JawanHaider/Documents/Projects/Collaborative/Dogar/flywasp-fd/notebooks/3.0-jh-initial-rnn-exploration.ipynb#W5sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m model_name \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmodel_architecture\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00minput_data\u001b[39m}\u001b[39;00m\u001b[39m_v\u001b[39m\u001b[39m{\u001b[39;00mversion_number\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/JawanHaider/Documents/Projects/Collaborative/Dogar/flywasp-fd/notebooks/3.0-jh-initial-rnn-exploration.ipynb#W5sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# Create the configuration dictionary\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/JawanHaider/Documents/Projects/Collaborative/Dogar/flywasp-fd/notebooks/3.0-jh-initial-rnn-exploration.ipynb#W5sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m config \u001b[39m=\u001b[39m create_config_dict(\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/JawanHaider/Documents/Projects/Collaborative/Dogar/flywasp-fd/notebooks/3.0-jh-initial-rnn-exploration.ipynb#W5sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     model_name\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mtimestamp\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mmodel_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/JawanHaider/Documents/Projects/Collaborative/Dogar/flywasp-fd/notebooks/3.0-jh-initial-rnn-exploration.ipynb#W5sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     input_size\u001b[39m=\u001b[39minput_size,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/JawanHaider/Documents/Projects/Collaborative/Dogar/flywasp-fd/notebooks/3.0-jh-initial-rnn-exploration.ipynb#W5sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     hidden_size\u001b[39m=\u001b[39mhidden_size,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/JawanHaider/Documents/Projects/Collaborative/Dogar/flywasp-fd/notebooks/3.0-jh-initial-rnn-exploration.ipynb#W5sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     output_size\u001b[39m=\u001b[39moutput_size,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/JawanHaider/Documents/Projects/Collaborative/Dogar/flywasp-fd/notebooks/3.0-jh-initial-rnn-exploration.ipynb#W5sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     num_epochs\u001b[39m=\u001b[39mnum_epochs,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/JawanHaider/Documents/Projects/Collaborative/Dogar/flywasp-fd/notebooks/3.0-jh-initial-rnn-exploration.ipynb#W5sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/JawanHaider/Documents/Projects/Collaborative/Dogar/flywasp-fd/notebooks/3.0-jh-initial-rnn-exploration.ipynb#W5sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     learning_rate\u001b[39m=\u001b[39mlearning_rate,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/JawanHaider/Documents/Projects/Collaborative/Dogar/flywasp-fd/notebooks/3.0-jh-initial-rnn-exploration.ipynb#W5sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     raw_data_path\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/JawanHaider/Documents/Projects/Collaborative/Dogar/flywasp-fd/notebooks/3.0-jh-initial-rnn-exploration.ipynb#W5sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     interim_data_path\u001b[39m=\u001b[39mpickle_path,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/JawanHaider/Documents/Projects/Collaborative/Dogar/flywasp-fd/notebooks/3.0-jh-initial-rnn-exploration.ipynb#W5sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     processed_data_path\u001b[39m=\u001b[39mprocessed_data_path,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/JawanHaider/Documents/Projects/Collaborative/Dogar/flywasp-fd/notebooks/3.0-jh-initial-rnn-exploration.ipynb#W5sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     logging_level\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mDEBUG\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/JawanHaider/Documents/Projects/Collaborative/Dogar/flywasp-fd/notebooks/3.0-jh-initial-rnn-exploration.ipynb#W5sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     logging_format\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%(asctime)s\u001b[39;00m\u001b[39m - \u001b[39m\u001b[39m%(levelname)s\u001b[39;00m\u001b[39m - \u001b[39m\u001b[39m%(module)s\u001b[39;00m\u001b[39m - \u001b[39m\u001b[39m%(message)s\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/JawanHaider/Documents/Projects/Collaborative/Dogar/flywasp-fd/notebooks/3.0-jh-initial-rnn-exploration.ipynb#W5sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m )  \u001b[39m# Create a dictionary with configuration settings\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/JawanHaider/Documents/Projects/Collaborative/Dogar/flywasp-fd/notebooks/3.0-jh-initial-rnn-exploration.ipynb#W5sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m# Save the trained model and configuration settings\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/JawanHaider/Documents/Projects/Collaborative/Dogar/flywasp-fd/notebooks/3.0-jh-initial-rnn-exploration.ipynb#W5sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m model_dir \u001b[39m=\u001b[39m Path(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmodels/\u001b[39m\u001b[39m{\u001b[39;00mmodel_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'timestamp' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the RNN model\n",
    "print(f\"Training RNN Model...\\n===============================\\n\")\n",
    "input_size = X_train.shape[2] # - 1  ### -1 because we drop the target column\n",
    "# print(f\"Input size: {input_size}\\n\\n\")\n",
    "hidden_size = 64\n",
    "output_size = 2\n",
    "num_epochs = 10\n",
    "batch_size = 512\n",
    "learning_rate = 0.001\n",
    "batch_first = True\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = train_model(X_train, Y_train, X_test, Y_test, input_size,\n",
    "                    hidden_size, output_size, num_epochs, batch_size, learning_rate, device, batch_first=batch_first)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_path = \"data/interim/ff-mw.pkl\"\n",
    "processed_data_path = \"data/processed/rnn_input/\"\n",
    "# Create the model name\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "model_architecture = \"rnn\"\n",
    "input_data = \"ff-mw\"\n",
    "version_number = 1\n",
    "model_name = f\"{model_architecture}_{input_data}_v{version_number}\"\n",
    "\n",
    "# Create the configuration dictionary\n",
    "config = create_config_dict(\n",
    "    model_name=f\"{timestamp}_{model_name}\",\n",
    "    input_size=input_size,\n",
    "    hidden_size=hidden_size,\n",
    "    output_size=output_size,\n",
    "    num_epochs=num_epochs,\n",
    "    batch_size=batch_size,\n",
    "    learning_rate=learning_rate,\n",
    "    raw_data_path=None,\n",
    "    interim_data_path=pickle_path,\n",
    "    processed_data_path=processed_data_path,\n",
    "    logging_level='DEBUG',\n",
    "    logging_format='%(asctime)s - %(levelname)s - %(module)s - %(message)s'\n",
    ")  # Create a dictionary with configuration settings\n",
    "\n",
    "# Save the trained model and configuration settings\n",
    "model_dir = Path(f\"models/{model_name}\")\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "config_dir = Path(f\"config/{model_name}\")\n",
    "config_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "save_model_and_config(model, model_name, timestamp, pickle_path, processed_data_path, config, model_dir, config_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 99.9%, Avg loss: 0.314837 \n",
      "\n",
      "F1 Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.metrics import f1_score\n",
    "# from torch.utils.data import DataLoader, Dataset\n",
    "# import torch.nn as nn\n",
    "# # Initialize a new model\n",
    "# input_size = X_test.shape[2]  # Make sure this is correct\n",
    "# hidden_size = 64\n",
    "# output_size = 2\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# new_model = rnn_model.RNN(input_size=input_size, hidden_size=hidden_size, output_size=output_size, batch_first=True).to(device)\n",
    "\n",
    "# # Load the model\n",
    "# model_path = \"models/rnn_ff-mw_v1/20231020_1701_model_2c92c2793be07eaf3765665d6287ded4_971fce5d8c82c2d1bf8db68939c8162d.pt\"\n",
    "# state_dict = torch.load(model_path)\n",
    "# new_model.load_state_dict(state_dict)\n",
    "# # loaded_model = torch.load(model_path)\n",
    "# new_model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# def evaluate_f1(model, X_test, Y_test, batch_size, device):\n",
    "#     test_dataset = rnn_model.WalkDataset(X_test, Y_test)\n",
    "#     test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "#     model.eval()\n",
    "\n",
    "#     # Initialize running loss & sum of squared gradients and parameters\n",
    "#     running_loss = 0.0\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "    \n",
    "#     y_true = []\n",
    "#     y_pred = []\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for i, (inputs, labels) in enumerate(test_loader):\n",
    "#             inputs, labels = inputs.to(device), labels.to(device)\n",
    "#             outputs = model(inputs)\n",
    "#             # Using CrossEntropyLoss as the loss function\n",
    "#             criterion = nn.CrossEntropyLoss()\n",
    "#             loss = criterion(outputs, labels)  # Compute loss\n",
    "#             running_loss += loss.item()  # Accumulate loss\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "#             total += labels.size(0)  # Accumulate total number of samples\n",
    "#             correct += (predicted == labels).sum().item()\n",
    "#             y_true.extend(labels.cpu().numpy().tolist())\n",
    "#             y_pred.extend(predicted.cpu().numpy().tolist())\n",
    "            \n",
    "#     # Calculate average loss and accuracy over all batches\n",
    "#     test_loss = running_loss / len(test_loader)\n",
    "#     test_acc = correct / total\n",
    "\n",
    "#     print(\n",
    "#         f\"Test Error: \\n Accuracy: {(100*test_acc):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    \n",
    "#     f1 = f1_score(y_true, y_pred)  # You can change the \"average\" parameter to suit your needs\n",
    "\n",
    "#     return f1\n",
    "\n",
    "# # Make sure you load your saved model into the variable `loaded_model`\n",
    "# # Also, ensure X_test, Y_test, batch_size and device are set\n",
    "\n",
    "# f1 = evaluate_f1(new_model, X_test, Y_test, batch_size, device)\n",
    "# print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fd_flywasp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
