{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import torch\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from src.data_preprocess.preprocessing import DataPreprocessor\n",
    "from src.data_preprocess.feature_engineering import FeatureEngineer\n",
    "from src.utils.utilities import prepare_train_test_sequences\n",
    "from src.utils.utilities import create_config_dict\n",
    "from src.utils.utilities import get_hash\n",
    "from src.utils.utilities import load_train_test_data\n",
    "from src.models.rnn_model import train_rnn_model\n",
    "import hashlib\n",
    "import pickle\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Performs preprocessing steps on the input DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The input DataFrame.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        The preprocessed DataFrame.\n",
    "    \"\"\"\n",
    "    preprocessor = DataPreprocessor(df=df)\n",
    "    preprocessor.drop_columns([\"plot\"])  # Drop the 'plot' column\n",
    "    preprocessor.calculate_means([[\"ANTdis_1\", \"ANTdis_2\"]], [\"ANTdis\"])  # Calculate the mean of 'ANTdis_1' and 'ANTdis_2' and store it in a new column 'ANTdis'\n",
    "    preprocessor.add_labels([\"walk_backwards\", \"walk_backwards\"], \"start_walk\")  # Add a new column 'start_walk' with value 'walk_backwards' for rows where the 'walk_backwards' column has value 'walk_backwards'\n",
    "    preprocessor.handle_infinity_and_na()  # Replace infinity and NaN values with appropriate values\n",
    "    preprocessor.specific_rearrange(\n",
    "        \"F2Wdis_rate\", \"F2Wdis\"\n",
    "    )  # Rearrange the column names\n",
    "    preprocessor.rearrange_columns(\n",
    "        [\n",
    "            \"Frame\",\n",
    "            \"Fdis\",\n",
    "            \"FdisF\",\n",
    "            \"FdisL\",\n",
    "            \"Wdis\",\n",
    "            \"WdisF\",\n",
    "            \"WdisL\",\n",
    "            \"Fangle\",\n",
    "            \"Wangle\",\n",
    "            \"F2Wdis\",\n",
    "            \"F2Wdis_rate\",\n",
    "            \"F2Wangle\",\n",
    "            \"W2Fangle\",\n",
    "            \"ANTdis\",\n",
    "            \"F2W_blob_dis\",\n",
    "            \"bp_F_delta\",\n",
    "            \"bp_W_delta\",\n",
    "            \"ap_F_delta\",\n",
    "            \"ap_W_delta\",\n",
    "            \"ant_W_delta\",\n",
    "            \"file\",\n",
    "            \"start_walk\",\n",
    "        ]\n",
    "    )  # Rearrange the columns in a specific order\n",
    "    return preprocessor.df\n",
    "\n",
    "\n",
    "def engineer_features(df):\n",
    "    \"\"\"\n",
    "    Performs feature engineering steps on the input DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The input DataFrame.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        The feature-engineered DataFrame.\n",
    "    \"\"\"\n",
    "    feature_engineer = FeatureEngineer(df=df)\n",
    "    feature_engineer.standardize_features(\n",
    "        [\n",
    "            \"Fdis\",\n",
    "            \"FdisF\",\n",
    "            \"FdisL\",\n",
    "            \"Wdis\",\n",
    "            \"WdisF\",\n",
    "            \"WdisL\",\n",
    "            \"Fangle\",\n",
    "            \"Wangle\",\n",
    "            \"F2Wdis\",\n",
    "            \"F2Wdis_rate\",\n",
    "            \"F2Wangle\",\n",
    "            \"W2Fangle\",\n",
    "            \"ANTdis\",\n",
    "            \"F2W_blob_dis\",\n",
    "            \"bp_F_delta\",\n",
    "            \"bp_W_delta\",\n",
    "            \"ap_F_delta\",\n",
    "            \"ap_W_delta\",\n",
    "            \"ant_W_delta\",\n",
    "        ]\n",
    "    )  # Standardize the selected features\n",
    "    return feature_engineer.df\n",
    "\n",
    "\n",
    "def train_model(X_train, Y_train, X_test, Y_test, input_size, hidden_size, output_size, num_epochs, batch_size, learning_rate, device, batch_first=True):\n",
    "    \"\"\"\n",
    "    Trains an RNN model on the input data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : numpy.ndarray\n",
    "        The training input sequences.\n",
    "    Y_train : numpy.ndarray\n",
    "        The training target sequences.\n",
    "    X_test : numpy.ndarray\n",
    "        The test input sequences.\n",
    "    Y_test : numpy.ndarray\n",
    "        The test target sequences.\n",
    "    input_size : int\n",
    "        The size of the input features.\n",
    "    hidden_size : int\n",
    "        The size of the hidden layer.\n",
    "    output_size : int\n",
    "        The size of the output layer.\n",
    "    num_epochs : int\n",
    "        The number of training epochs.\n",
    "    batch_size : int\n",
    "        The batch size for training.\n",
    "    learning_rate : float\n",
    "        The learning rate for training.\n",
    "    device : torch.device\n",
    "        The device to use for training.\n",
    "    batch_first : bool, optional\n",
    "        Whether the input sequences have the batch dimension as the first dimension.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.nn.Module\n",
    "        The trained RNN model.\n",
    "    \"\"\"\n",
    "    model = train_rnn_model(X_train, Y_train, X_test, Y_test, input_size,\n",
    "                            hidden_size, output_size, num_epochs, batch_size, learning_rate, device, batch_first=batch_first)  # Train the RNN model\n",
    "    return model\n",
    "\n",
    "\n",
    "def save_model_and_config(model, model_name, timestamp, pickle_path, processed_data_path, config, model_dir, config_dir):\n",
    "    \"\"\"\n",
    "    Saves the trained model and configuration settings.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : torch.nn.Module\n",
    "        The trained RNN model.\n",
    "    model_name : str\n",
    "        The name of the model.\n",
    "    timestamp : str\n",
    "        The timestamp to use in the output file names.\n",
    "    pickle_path : str\n",
    "        The path to the input data pickle file.\n",
    "    processed_data_path : str\n",
    "        The path to the processed data pickle file.\n",
    "    config : dict\n",
    "        The configuration settings for the model.\n",
    "    model_dir : pathlib.Path\n",
    "        The directory to save the trained model.\n",
    "    config_dir : pathlib.Path\n",
    "        The directory to save the configuration settings.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Get the hash values of the model and configuration\n",
    "    model_hash = hashlib.md5(str(model.state_dict()).encode('utf-8')).hexdigest()\n",
    "    config_hash = hashlib.md5(str(config).encode('utf-8')).hexdigest()\n",
    "\n",
    "    # Check if the model and configuration already exist\n",
    "    existing_models = [f.name for f in model_dir.glob(\"*.pt\")]\n",
    "    existing_configs = [f.name for f in config_dir.glob(\"*.yaml\")]\n",
    "    if f\"rnn_model_{model_hash}.pt\" in existing_models and f\"config_{config_hash}.yaml\" in existing_configs:\n",
    "        logging.info(\"Model and configuration already exist. Skipping saving.\")\n",
    "    else:\n",
    "        # Save the trained model\n",
    "        model_path = model_dir / \\\n",
    "            f\"{timestamp}_model_{model_hash}_{config_hash}.pt\"\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "        # Save the configuration settings\n",
    "        config_path = config_dir / f\"{timestamp}_config_{config_hash}.yaml\"\n",
    "        with open(config_path, \"w\") as f:\n",
    "            yaml.dump(config, f)\n",
    "\n",
    "def save_train_test_data(X_train, Y_train, X_test, Y_test):\n",
    "    \"\"\"\n",
    "    Saves the train and test datasets for the RNN model as .pkl files.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : numpy.ndarray\n",
    "        The training input sequences.\n",
    "    Y_train : numpy.ndarray\n",
    "        The training target values.\n",
    "    X_test : numpy.ndarray\n",
    "        The testing input sequences.\n",
    "    Y_test : numpy.ndarray\n",
    "        The testing target values.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create a timestamped directory for the processed data\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d\")\n",
    "        dir_name = Path(f\"data/processed/rnn_input/{timestamp}\")\n",
    "        dir_name.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Save the train and test datasets as .pkl files\n",
    "        X_train_file = dir_name / \"X_train.pkl\"\n",
    "        Y_train_file = dir_name / \"Y_train.pkl\"\n",
    "        X_test_file = dir_name / \"X_test.pkl\"\n",
    "        Y_test_file = dir_name / \"Y_test.pkl\"\n",
    "\n",
    "        with open(X_train_file, \"wb\") as f:\n",
    "            pickle.dump(X_train, f)\n",
    "        with open(Y_train_file, \"wb\") as f:\n",
    "            pickle.dump(Y_train, f)\n",
    "        with open(X_test_file, \"wb\") as f:\n",
    "            pickle.dump(X_test, f)\n",
    "        with open(Y_test_file, \"wb\") as f:\n",
    "            pickle.dump(Y_test, f)\n",
    "\n",
    "        logging.info(f\"Saved train and test datasets to {dir_name}.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving train and test datasets: {e}\")\n",
    "        raise\n",
    "    return dir_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_load = True\n",
    "if is_load:\n",
    "    X_train, Y_train, X_test, Y_test = load_train_test_data('../data/processed/rnn_input/')\n",
    "else:\n",
    "    # Initialize preprocessing object and load data\n",
    "    pickle_path = \"data/interim/ff-mw.pkl\"\n",
    "    preprocessor = DataPreprocessor(pickle_path=pickle_path)\n",
    "    logging.info(\"Loading data...\")\n",
    "    df = preprocessor.load_data()\n",
    "\n",
    "    # Perform preprocessing steps\n",
    "    logging.info(\"Performing preprocessing steps...\")\n",
    "    df = preprocess_data(df)\n",
    "\n",
    "    # Perform feature engineering steps\n",
    "    logging.info(\"Performing feature engineering steps...\")\n",
    "    df = engineer_features(df)\n",
    "\n",
    "    # Save the processed data\n",
    "    logging.info(\"Saving processed data...\")\n",
    "    input_data = \"ff-mw\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "    processed_data_path = preprocessor.save_processed_data(input_data, timestamp)  # Save the processed data to a file\n",
    "\n",
    "    # Prepare sequences and train-test splits\n",
    "    logging.info(\"Preparing sequences and train-test splits...\")\n",
    "    X_train, Y_train, X_test, Y_test = prepare_train_test_sequences(df)\n",
    "\n",
    "    # Save the train-test splits\n",
    "    logging.info(\"Saving train-test splits...\")\n",
    "    save_train_test_data(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Training RNN model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: Train Loss: nan, Test Loss: nan, Test Acc: 0.9991\n",
      "Epoch 2/10: Train Loss: nan, Test Loss: nan, Test Acc: 0.9991\n",
      "Epoch 3/10: Train Loss: nan, Test Loss: nan, Test Acc: 0.9991\n",
      "Epoch 4/10: Train Loss: nan, Test Loss: nan, Test Acc: 0.9991\n",
      "Epoch 5/10: Train Loss: nan, Test Loss: nan, Test Acc: 0.9991\n",
      "Epoch 6/10: Train Loss: nan, Test Loss: nan, Test Acc: 0.9991\n",
      "Epoch 7/10: Train Loss: nan, Test Loss: nan, Test Acc: 0.9991\n",
      "Epoch 8/10: Train Loss: nan, Test Loss: nan, Test Acc: 0.9991\n",
      "Epoch 9/10: Train Loss: nan, Test Loss: nan, Test Acc: 0.9991\n",
      "Epoch 10/10: Train Loss: nan, Test Loss: nan, Test Acc: 0.9991\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'input_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/JawanHaider/Documents/Projects/Collaborative/Dogar/flywasp-fd/notebooks/3.0-jh-initial-rnn-exploration.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/JawanHaider/Documents/Projects/Collaborative/Dogar/flywasp-fd/notebooks/3.0-jh-initial-rnn-exploration.ipynb#W3sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m model_architecture \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrnn\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/JawanHaider/Documents/Projects/Collaborative/Dogar/flywasp-fd/notebooks/3.0-jh-initial-rnn-exploration.ipynb#W3sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m version_number \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/JawanHaider/Documents/Projects/Collaborative/Dogar/flywasp-fd/notebooks/3.0-jh-initial-rnn-exploration.ipynb#W3sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m model_name \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmodel_architecture\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00minput_data\u001b[39m}\u001b[39;00m\u001b[39m_v\u001b[39m\u001b[39m{\u001b[39;00mversion_number\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/JawanHaider/Documents/Projects/Collaborative/Dogar/flywasp-fd/notebooks/3.0-jh-initial-rnn-exploration.ipynb#W3sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# Create the configuration dictionary\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/JawanHaider/Documents/Projects/Collaborative/Dogar/flywasp-fd/notebooks/3.0-jh-initial-rnn-exploration.ipynb#W3sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m config \u001b[39m=\u001b[39m create_config_dict(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/JawanHaider/Documents/Projects/Collaborative/Dogar/flywasp-fd/notebooks/3.0-jh-initial-rnn-exploration.ipynb#W3sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     model_name\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mtimestamp\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mmodel_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/JawanHaider/Documents/Projects/Collaborative/Dogar/flywasp-fd/notebooks/3.0-jh-initial-rnn-exploration.ipynb#W3sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     input_size\u001b[39m=\u001b[39minput_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/JawanHaider/Documents/Projects/Collaborative/Dogar/flywasp-fd/notebooks/3.0-jh-initial-rnn-exploration.ipynb#W3sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     logging_format\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%(asctime)s\u001b[39;00m\u001b[39m - \u001b[39m\u001b[39m%(levelname)s\u001b[39;00m\u001b[39m - \u001b[39m\u001b[39m%(module)s\u001b[39;00m\u001b[39m - \u001b[39m\u001b[39m%(message)s\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/JawanHaider/Documents/Projects/Collaborative/Dogar/flywasp-fd/notebooks/3.0-jh-initial-rnn-exploration.ipynb#W3sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m )  \u001b[39m# Create a dictionary with configuration settings\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_data' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the RNN model\n",
    "logging.info(\"Training RNN model...\")\n",
    "input_size = X_train.shape[2] # - 1  # -1 because we drop the target column\n",
    "hidden_size = 64\n",
    "output_size = 2\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "batch_first = True\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = train_model(X_train, Y_train, X_test, Y_test, input_size,\n",
    "                    hidden_size, output_size, num_epochs, batch_size, learning_rate, device, batch_first=batch_first)\n",
    "\n",
    "# Create the model name\n",
    "model_architecture = \"rnn\"\n",
    "version_number = 1\n",
    "model_name = f\"{model_architecture}_{input_data}_v{version_number}\"\n",
    "\n",
    "# Create the configuration dictionary\n",
    "config = create_config_dict(\n",
    "    model_name=f\"{timestamp}_{model_name}\",\n",
    "    input_size=input_size,\n",
    "    hidden_size=hidden_size,\n",
    "    output_size=output_size,\n",
    "    num_epochs=num_epochs,\n",
    "    batch_size=batch_size,\n",
    "    learning_rate=learning_rate,\n",
    "    raw_data_path=None,\n",
    "    interim_data_path=pickle_path,\n",
    "    processed_data_path=processed_data_path,\n",
    "    logging_level='DEBUG',\n",
    "    logging_format='%(asctime)s - %(levelname)s - %(module)s - %(message)s'\n",
    ")  # Create a dictionary with configuration settings\n",
    "\n",
    "# Save the trained model and configuration settings\n",
    "model_dir = Path(f\"models/{model_name}\")\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "config_dir = Path(f\"config/{model_name}\")\n",
    "config_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "save_model_and_config(model, model_name, timestamp, pickle_path, processed_data_path, config, model_dir, config_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fd_flywasp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
